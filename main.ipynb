{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idiom Detection with BERT + BiLSTM + CRF\n",
    "This notebook is adapted from your `main.py` for use in Google Colab.\n",
    "\n",
    "**Instructions:**\n",
    "- Upload your `model.py` and `dataset.py` files to the Colab environment.\n",
    "- Place your data in the appropriate paths (e.g., `public_data/`, `starting_kit/`).\n",
    "- Use the cells below to train, evaluate, or predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers tqdm scikit-learn pytorch-crf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dataset import get_dataloaders\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from biltsm_crf_model import (\n",
    "    EnhancedBertForIdiomDetection,\n",
    "    train_model,\n",
    "    predict_idioms_with_postprocessing,\n",
    "    evaluate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(epochs=10, lr=2e-5, batch_size=8, max_length=128, \n",
    "              lstm_hidden_size=384, lstm_layers=2, lstm_dropout=0.3,\n",
    "              hidden_dropout=0.3, use_layer_norm=True, freeze_bert_layers=0):\n",
    "    # ... existing dataloader code ...\n",
    "    \n",
    "    model = EnhancedBertForIdiomDetection(\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_layers=lstm_layers,\n",
    "        lstm_dropout=lstm_dropout,\n",
    "        hidden_dropout=hidden_dropout,\n",
    "        use_layer_norm=use_layer_norm,\n",
    "        freeze_bert_layers=freeze_bert_layers\n",
    "    )\n",
    "    \n",
    "    model = train_model(\n",
    "        train_loader, val_loader, tokenizer,\n",
    "        model=model,\n",
    "        epochs=epochs,\n",
    "        lr=lr\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(batch_size=8, max_length=128, \n",
    "            lstm_hidden_size=384, lstm_layers=2, lstm_dropout=0.3,\n",
    "            hidden_dropout=0.3, use_layer_norm=True, freeze_bert_layers=0):\n",
    "    # ... existing device and dataloader code ...\n",
    "    \n",
    "    model = EnhancedBertForIdiomDetection(\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_layers=lstm_layers,\n",
    "        lstm_dropout=lstm_dropout,\n",
    "        hidden_dropout=hidden_dropout,\n",
    "        use_layer_norm=use_layer_norm,\n",
    "        freeze_bert_layers=freeze_bert_layers\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "def run_predict(output='predictions.csv', \n",
    "               lstm_hidden_size=384, lstm_layers=2, lstm_dropout=0.3,\n",
    "               hidden_dropout=0.3, use_layer_norm=True, freeze_bert_layers=0):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model with same parameters as training\n",
    "    model = EnhancedBertForIdiomDetection(\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_layers=lstm_layers,\n",
    "        lstm_dropout=lstm_dropout,\n",
    "        hidden_dropout=hidden_dropout,\n",
    "        use_layer_norm=use_layer_norm,\n",
    "        freeze_bert_layers=freeze_bert_layers\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_idiom_model.pt', map_location=device))\n",
    "    model.to(device)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    # Read test data\n",
    "    test_df = pd.read_csv('starting_kit/eval_w_o_labels.csv')\n",
    "    ids = test_df['id'].tolist()\n",
    "    sentences = test_df['sentence'].tolist()\n",
    "    languages = test_df['language'].tolist()\n",
    "\n",
    "    results = []\n",
    "    for idx, sentence, lang in zip(ids, sentences, languages):\n",
    "        # Use the new prediction function with post-processing\n",
    "        idiom_indices = predict_idioms_with_postprocessing(model, tokenizer, sentence, device)\n",
    "        \n",
    "        # If no idiom is found, use [-1] as per the competition format\n",
    "        if not idiom_indices:\n",
    "            idiom_indices = [-1]\n",
    "            \n",
    "        results.append({\n",
    "            'id': idx,\n",
    "            'indices': str(idiom_indices),\n",
    "            'language': lang\n",
    "        })\n",
    "\n",
    "    out_df = pd.DataFrame(results)\n",
    "    out_df.to_csv(output, index=False)\n",
    "    print(f'Predictions saved to {output}')\n",
    "\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "Uncomment and run the cell below for the desired operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# run_train(epochs=10, lr=2e-5, batch_size=8, max_length=128)\n",
    "\n",
    "# Evaluate\n",
    "# run_eval(batch_size=8, max_length=128)\n",
    "\n",
    "# Predict\n",
    "# run_predict(output='predictions.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
