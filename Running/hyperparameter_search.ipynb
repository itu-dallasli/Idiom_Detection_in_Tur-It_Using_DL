{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hyperparameter Searching Idea for Bert-based Model*\n",
    "\n",
    "Creating some random choices in the terms of parameters. Create multiple configurations randomly, train a few epochs for each configuration and compare the results. Finally, obtain the best hyperparameter configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparameter_search(n_trials=5, batch_size=8, max_length=128):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Get data loaders\n",
    "    train_loader, val_loader, _ = get_dataloaders(\n",
    "        batch_size=batch_size, \n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    \n",
    "    # Define the hyperparameter search space\n",
    "    param_grid = {\n",
    "        'learning_rate': [1e-5, 2e-5, 3e-5, 5e-5],\n",
    "        'lstm_hidden_size': [256, 384, 512],\n",
    "        'lstm_layers': [1, 2, 3],\n",
    "        'lstm_dropout': [0.2, 0.3, 0.4],\n",
    "        'hidden_dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "        'use_layer_norm': [True, False],\n",
    "        'freeze_bert_layers': [0, 3, 6],\n",
    "        'weight_decay': [0.0, 0.01, 0.05],\n",
    "        'lr_multiplier': [5, 10, 15]\n",
    "    }\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_config = None\n",
    "    results = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        print(f\"\\n===== Trial {trial+1}/{n_trials} =====\")\n",
    "        \n",
    "        # Sample random hyperparameters and convert to Python native types\n",
    "        config = {\n",
    "            'learning_rate': float(np.random.choice(param_grid['learning_rate'])),\n",
    "            'lstm_hidden_size': int(np.random.choice(param_grid['lstm_hidden_size'])),\n",
    "            'lstm_layers': int(np.random.choice(param_grid['lstm_layers'])),\n",
    "            'lstm_dropout': float(np.random.choice(param_grid['lstm_dropout'])),\n",
    "            'hidden_dropout': float(np.random.choice(param_grid['hidden_dropout'])),\n",
    "            'use_layer_norm': bool(np.random.choice(param_grid['use_layer_norm'])),\n",
    "            'freeze_bert_layers': int(np.random.choice(param_grid['freeze_bert_layers'])),\n",
    "            'weight_decay': float(np.random.choice(param_grid['weight_decay'])),\n",
    "            'lr_multiplier': int(np.random.choice(param_grid['lr_multiplier']))\n",
    "        }\n",
    "        \n",
    "        print(\"Configuration:\")\n",
    "        for k, v in config.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        \n",
    "        # Initialize model with sampled hyperparameters\n",
    "        model = EnhancedBertForIdiomDetection(\n",
    "            lstm_hidden_size=config['lstm_hidden_size'],\n",
    "            lstm_layers=config['lstm_layers'],\n",
    "            lstm_dropout=config['lstm_dropout'],\n",
    "            hidden_dropout=config['hidden_dropout'],\n",
    "            use_layer_norm=config['use_layer_norm'],\n",
    "            freeze_bert_layers=config['freeze_bert_layers']\n",
    "        )\n",
    "        \n",
    "        # Train for a few epochs to evaluate the configuration\n",
    "        trial_epochs = 3  # Lower the epoch, faster the evaluation\n",
    "        model = train_model(\n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            tokenizer,\n",
    "            model=model,\n",
    "            epochs=trial_epochs,\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            lr_multiplier=config['lr_multiplier'],\n",
    "            patience=2  # Use shorter patience for hyperparameter search\n",
    "        )\n",
    "        \n",
    "        # Evaluate with post-processing\n",
    "        metrics = evaluate(model, val_loader, tokenizer, device, apply_postprocessing=True)\n",
    "        f1_score = metrics['f1']\n",
    "        \n",
    "        print(f\"Trial {trial+1} F1 Score: {f1_score:.4f}\")\n",
    "        \n",
    "        # Save results\n",
    "        config['f1_score'] = f1_score\n",
    "        results.append(config)\n",
    "        \n",
    "        # Update best configuration\n",
    "        if f1_score > best_f1:\n",
    "            best_f1 = f1_score\n",
    "            best_config = config\n",
    "            print(f\"New best configuration found! F1: {best_f1:.4f}\")\n",
    "    \n",
    "    # Print results summary\n",
    "    print(\"\\n===== Hyperparameter Search Results =====\")\n",
    "    print(f\"Best F1 Score: {best_f1:.4f}\")\n",
    "    print(\"Best Configuration:\")\n",
    "    for k, v in best_config.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Sort all results by F1 score\n",
    "    results.sort(key=lambda x: x['f1_score'], reverse=True)\n",
    "    print(\"\\nTop 3 configurations:\")\n",
    "    for i, config in enumerate(results[:3]):\n",
    "        print(f\"Rank {i+1}: F1={config['f1_score']:.4f}\")\n",
    "        for k, v in config.items():\n",
    "            if k != 'f1_score':\n",
    "                print(f\"  {k}: {v}\")\n",
    "    \n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
