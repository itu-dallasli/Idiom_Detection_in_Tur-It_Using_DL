# ğŸ¯ Idiom Detection Project

This repository contains a machine learning and deep learning hybrid project focused on idiom detection in multiple languages, specifically Turkish and Italian. The project uses advanced NLP techniques and transformer-based models to identify idioms in text.

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ ğŸ“‚ Config/                 # Configuration files
â”œâ”€â”€ ğŸ“‚ Dataset/                # Dataset files
â”œâ”€â”€ ğŸ“‚ Models/                 # Model implementations and saved models
â”‚   â”œâ”€â”€ ğŸ“‚ Transformer/    # Transformer
â”‚   â”œâ”€â”€ ğŸ“‚ Latest_BiLSTM-CRF/  # BiLSTM-CRF model implementation
â”‚   â””â”€â”€ ğŸ“„ xlm_roberta_improved.py  # Enhanced XLM-RoBERTa model
â”œâ”€â”€ ğŸ“‚ Submissions/            # Submission files
â”œâ”€â”€ ğŸ“‚ Running/                # IPYNB files and main running files
â”‚   â”œâ”€â”€ ğŸ“„ xlm_earlystop_main.ipynb  # Main training notebook
â”‚   â””â”€â”€ ğŸ“„ hyperparameter_search.ipynb  # Hyperparameter optimization
â””â”€â”€ ğŸ“„ requirements.txt        # Project dependencies
```

## ğŸš€ Quick Start

### 1. Environment Setup
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“Š Data Format

### Input Format ğŸ“¥
The models expect CSV files with the following columns:
| Column | Description |
|--------|-------------|
| `id` | Unique identifier for each sentence |
| `text`/`sentence` | The input text |
| `language` | Language code |
| `labels` | BIO tags for training data |
| `indices` | List of indices where MWEs are located |

### Output Format ğŸ“¤
The models produce a CSV file with:
| Column | Description |
|--------|-------------|
| `id` | Original sentence ID |
| `indices` | List of word indices that form idioms |
| `language` | Original language code |

## ğŸ¤– Model Architectures

### 1. ğŸ§  XLM-RoBERTa Model (Enhanced)
- Pre-trained XLM-RoBERTa large model
- BiLSTM layers for capturing long-range context information
- Conditional Random Fields (CRF) for optimal sequence labeling
- Layer normalization and configurable dropout
- Detailed evaluation metrics calculation

### 2. ğŸ”„ Transformer MLM Model
- Transformer-based architecture with Masked Language Modeling
- Enhanced context understanding through MLM pre-training
- Multi-head attention for MWE detection
- CRF layer for sequence labeling
- Flexible architecture for different language pairs

### 3. ğŸ“ˆ BiLSTM-CRF Model
- BERT/RoBERTa as the base encoder
- Bidirectional LSTM for sequence modeling
- CRF layer for structured prediction
- Focal Loss for handling class imbalance
- Configurable architecture parameters

## ğŸƒâ€â™‚ï¸ Running the Models

### Training

1. "Running/" folder contains example ipynb files to show how to use the models:  
```bash
jupyter notebook Running/xlm_earlystop_main.ipynb    # For XLM-RoBERTa model
jupyter notebook Running/hyperparameter_search.ipynb  # Plug-in function definition for hyperparameter optimization in the training step.
```

### Transformer MLM Model Usage

The repository provides a unified `run.py` script that can handle both training and prediction:

#### ğŸ¯ Training Mode
```bash
python run.py --mode train \
    --train_path path/to/train.csv \
    --val_path path/to/val.csv \
    --output_dir path/to/output
```

#### ğŸ”® Prediction Mode
```bash
python run.py --mode predict \
    --test_path path/to/test.csv \
    --output_dir path/to/output \
    --model_path path/to/model.pt
```

#### ğŸ”„ Both Training and Prediction
```bash
python run.py --mode both \
    --train_path path/to/train.csv \
    --val_path path/to/val.csv \
    --test_path path/to/test.csv \
    --output_dir path/to/output
```

### BiLSTM-CRF Model Usage

#### ğŸ¯ Training
```bash
python run_train.py \
    --train_csv path/to/train.csv \
    --output_dir path/to/save/model \
    --model_name bert-base-multilingual-cased \
    --epochs 5 \
    --batch_size 16 \
    --seed 42
```

#### ğŸ”® Inference
```bash
python run_predict.py \
    --model_dir path/to/saved/model \
    --test_csv path/to/test.csv \
    --output_csv predictions.csv \
    --seed 42
```

## âš™ï¸ Hyperparameter Optimization

The project includes a comprehensive hyperparameter search framework that optimizes:
- ğŸ“Š Learning rate
- ğŸ§  LSTM hidden size and layers
- ğŸ’§ Dropout rates
- ğŸ“ Layer normalization
- ğŸ”’ BERT layer freezing
- âš–ï¸ Weight decay
- ğŸ“ˆ Learning rate multiplier

## ğŸ“Š Evaluation

Consider metrics and structure by the main competition files.
Use the scoring script to evaluate model predictions:
```bash
python scoring.py
```

The model is evaluated using F1-score, calculated separately for:
- ğŸ‡¹ğŸ‡· Turkish language (f1-score-tr)
- ğŸ‡®ğŸ‡¹ Italian language (f1-score-it)
- ğŸŒ Average score across languages (f1-score-avg)

## ğŸ”„ Reproducibility

The code includes:
- ğŸ² Fixed random seeds for reproducibility
- ğŸ” Deterministic operations where possible
- ğŸ“¦ Version-controlled dependencies
- ğŸ”„ Consistent data preprocessing

## ğŸ“š External Resources

The models use the following pre-trained models:
- ğŸ¤– XLM-RoBERTa base/large model
- ğŸŒ bert-base-multilingual-cased
- ğŸ”¤ xlm-roberta-base

These are automatically downloaded when first used.

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b YZV405_2425_150220331_150210310`)
3. Commit your changes (`git commit -m 'Add some YZV405_2425_150220331_150210310'`)
4. Push to the branch (`git push origin YZV405_2425_150220331_150210310`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- ğŸ¤— Hugging Face Transformers library
