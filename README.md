# ğŸ¯ Idiom Detection Project

This repository contains a machine learning and deep learning hybrid project focused on idiom detection in multiple languages, specifically Turkish and Italian. The project uses advanced NLP techniques and transformer-based models to identify idioms in text.

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ ğŸ“‚ Config/                 # Configuration files
â”œâ”€â”€ ğŸ“‚ Dataset/                # Dataset files
â”œâ”€â”€ ğŸ“‚ Models/                 # Model implementations and saved models
â”‚   â”œâ”€â”€ ğŸ“‚ Transformer/    # Transformer
â”‚   â”œâ”€â”€ ğŸ“‚ Latest_BiLSTM-CRF/  # BiLSTM-CRF model implementation
â”‚   â”œâ”€â”€ ğŸ“‚ Roberta/       # Roberta model implementation
â”‚   â””â”€â”€ ğŸ“„ xlm_roberta_improved.py  # Enhanced XLM-RoBERTa model
â”œâ”€â”€ ğŸ“‚ Submissions/            # Submission files
â”œâ”€â”€ ğŸ“‚ Running/                # IPYNB files and main running files
â”‚   â”œâ”€â”€ ğŸ“„ xlm_earlystop_main.ipynb  # Main training notebook
â”‚   â””â”€â”€ ğŸ“„ hyperparameter_search.ipynb  # Hyperparameter optimization
â””â”€â”€ ğŸ“„ requirements.txt        # Project dependencies
```

## ğŸš€ Quick Start

### 1. Environment Setup
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Model Weights
The pre-trained model weights can be downloaded from [Google Drive](https://drive.google.com/drive/u/0/folders/1Ye8jMhutMpNkB-JPU5LCxYAhUkkoAQ0j). Place the downloaded weights in the `Models/saved_models/` directory. You can download the model which name is `xlm_bs16_lr3e-05_ml128_ep5.pt`.

## ğŸ“Š Data Format

### Input Format ğŸ“¥
The models expect CSV files with the following columns:
| Column | Description |
|--------|-------------|
| `id` | Unique identifier for each sentence |
| `text`/`sentence` | The input text |
| `language` | Language code |
| `labels` | BIO tags for training data |
| `indices` | List of indices where MWEs are located |
| `tokenized_sentence` | (For training data) List of tokenized words |

### Output Format ğŸ“¤
The models produce a CSV file with:
| Column | Description |
|--------|-------------|
| `id` | Original sentence ID |
| `indices` | List of word indices that form idioms |
| `language` | Original language code |

## ğŸ¤– Model Architectures

### 1. ğŸ§  XLM-RoBERTa Model (Enhanced)
- Pre-trained XLM-RoBERTa large model
- BiLSTM layers for capturing long-range context information
- Conditional Random Fields (CRF) for optimal sequence labeling
- Layer normalization and configurable dropout
- Detailed evaluation metrics calculation

#### Training Parameters
```bash
python train.py --data_dir dataset --models_dir Models --output_dir Submissions --batch_size 16 --learning_rate 3e-5 --max_length 128 --epochs 5
```

Optional arguments:
```
Path arguments:
  --data_dir DATA_DIR     Directory containing the datasets (default: dataset)
  --models_dir MODELS_DIR Directory to save models (default: Models)
  --output_dir OUTPUT_DIR Directory to save outputs (default: Submissions)

Model parameters:
  --model_name MODEL_NAME Base model name (default: xlm-roberta-large)
  --num_labels NUM_LABELS Number of labels for classification (default: 3)
  --hidden_size HIDDEN_SIZE Size of the LSTM hidden layer (default: 256)
  --dropout DROPOUT       Dropout rate (default: 0.2)

Training parameters:
  --batch_size BATCH_SIZE Batch size for training (default: 16)
  --learning_rate LEARNING_RATE Learning rate (default: 3e-5)
  --max_length MAX_LENGTH Maximum sequence length (default: 128)
  --epochs EPOCHS         Number of training epochs (default: 15)
  --seed SEED             Random seed for reproducibility (default: 42)
  --weight_decay WEIGHT_DECAY Weight decay for AdamW optimizer (default: 0.01)
  --warmup_ratio WARMUP_RATIO Ratio of warmup steps (default: 0.1)
  --patience PATIENCE     Patience for early stopping (default: 2)
  --max_grad_norm MAX_GRAD_NORM Maximum gradient norm for clipping (default: 1.0)
  --cpu                   Use CPU even if CUDA is available
  --experiment_name EXPERIMENT_NAME Custom name for the experiment
```

#### Inference
```bash
python run.py --model_path Models/saved_models/MODEL_NAME.pt --input_file dataset/test_w_o_labels.csv --output_file Submissions/predictions.csv
```

Required arguments:
```
--model_path MODEL_PATH   Path to the trained model weights
--input_file INPUT_FILE   Path to the input test file (CSV format)
--output_file OUTPUT_FILE Path to save predictions (CSV format)
```

Optional arguments:
```
--model_name MODEL_NAME   Base model name to use (default: xlm-roberta-large)
--num_labels NUM_LABELS   Number of labels for classification (default: 3)
--seed SEED               Random seed for reproducibility (default: 42)
--device {cuda,cpu}       Device to run on (default: auto-detect)
```

### 2. ğŸ”„ Transformer MLM Model
- Transformer-based architecture with Masked Language Modeling
- Enhanced context understanding through MLM pre-training
- Multi-head attention for MWE detection
- CRF layer for sequence labeling
- Flexible architecture for different language pairs

### 3. ğŸ“ˆ BiLSTM-CRF Model
- BERT/RoBERTa as the base encoder
- Bidirectional LSTM for sequence modeling
- CRF layer for structured prediction
- Focal Loss for handling class imbalance
- Configurable architecture parameters

## ğŸƒâ€â™‚ï¸ Running the Models

### Training

1. "Running/" folder contains example ipynb files to show how to use the models:  
```bash
jupyter notebook Running/xlm_earlystop_main.ipynb    # For XLM-RoBERTa model
jupyter notebook Running/hyperparameter_search.ipynb  # Plug-in function definition for hyperparameter optimization in the training step.
```

### Transformer MLM Model Usage

The repository provides a unified `run.py` script that can handle both training and prediction:

#### ğŸ¯ Training Mode
```bash
python run.py --mode train \
    --train_path path/to/train.csv \
    --val_path path/to/val.csv \
    --output_dir path/to/output
```

#### ğŸ”® Prediction Mode
```bash
python run.py --mode predict \
    --test_path path/to/test.csv \
    --output_dir path/to/output \
    --model_path path/to/model.pt
```

#### ğŸ”„ Both Training and Prediction
```bash
python run.py --mode both \
    --train_path path/to/train.csv \
    --val_path path/to/val.csv \
    --test_path path/to/test.csv \
    --output_dir path/to/output
```

### BiLSTM-CRF Model Usage

#### ğŸ¯ Training
```bash
python run_train.py \
    --train_csv path/to/train.csv \
    --output_dir path/to/save/model \
    --model_name bert-base-multilingual-cased \
    --epochs 5 \
    --batch_size 16 \
    --seed 42
```

#### ğŸ”® Inference
```bash
python run_predict.py \
    --model_dir path/to/saved/model \
    --test_csv path/to/test.csv \
    --output_csv predictions.csv \
    --seed 42
```

## âš™ï¸ Hyperparameter Optimization

The project includes a comprehensive hyperparameter search framework that optimizes:
- ğŸ“Š Learning rate
- ğŸ§  LSTM hidden size and layers
- ğŸ’§ Dropout rates
- ğŸ“ Layer normalization
- ğŸ”’ BERT layer freezing
- âš–ï¸ Weight decay
- ğŸ“ˆ Learning rate multiplier

## ğŸ“Š Evaluation

Consider metrics and structure by the main competition files.
Use the scoring script to evaluate model predictions:
```bash
python scoring.py
```

The model is evaluated using F1-score, calculated separately for:
- ğŸ‡¹ğŸ‡· Turkish language (f1-score-tr)
- ğŸ‡®ğŸ‡¹ Italian language (f1-score-it)
- ğŸŒ Average score across languages (f1-score-avg)

## ğŸ”„ Reproducibility

The code includes:
- ğŸ² Fixed random seeds for reproducibility
- ğŸ” Deterministic operations where possible
- ğŸ“¦ Version-controlled dependencies
- ğŸ”„ Consistent data preprocessing
- CUDA operations are set to be deterministic (may impact performance)
- For exact reproducibility across runs, use the `--seed` parameter with the same value
- When running on the same hardware with the same dependencies and seeds, results should be consistent

## ğŸ“š External Resources

The models use the following pre-trained models:
- ğŸ¤– XLM-RoBERTa base/large model
- ğŸŒ bert-base-multilingual-cased
- ğŸ”¤ xlm-roberta-base

These are automatically downloaded when first used.

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b YZV405_2425_150220331_150210310`)
3. Commit your changes (`git commit -m 'Add some YZV405_2425_150220331_150210310'`)
4. Push to the branch (`git push origin YZV405_2425_150220331_150210310`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- ğŸ¤— Hugging Face Transformers library
