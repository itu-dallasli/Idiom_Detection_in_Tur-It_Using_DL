{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataset import get_dataloaders\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchcrf import CRF\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define the Enhanced Model\n",
    "class EnhancedBertForIdiomDetection(nn.Module):\n",
    "    def __init__(self, \n",
    "                 model_name=\"bert-base-multilingual-cased\", \n",
    "                 num_labels=3,\n",
    "                 lstm_hidden_size=384,\n",
    "                 lstm_layers=2,\n",
    "                 lstm_dropout=0.3,\n",
    "                 hidden_dropout=0.3,\n",
    "                 use_layer_norm=True,\n",
    "                 freeze_bert_layers=0):\n",
    "        super(EnhancedBertForIdiomDetection, self).__init__()\n",
    "        \n",
    "        # Pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze specified number of BERT layers if needed\n",
    "        if freeze_bert_layers > 0:\n",
    "            modules = [self.bert.embeddings]\n",
    "            modules.extend(self.bert.encoder.layer[:freeze_bert_layers])\n",
    "            for module in modules:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # Add a BiLSTM layer to capture context\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.bert.config.hidden_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=lstm_dropout if lstm_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Classification layers\n",
    "        self.dropout = nn.Dropout(hidden_dropout)\n",
    "        self.dense = nn.Linear(lstm_hidden_size*2, lstm_hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        if use_layer_norm:\n",
    "            self.norm = nn.LayerNorm(lstm_hidden_size)\n",
    "        self.classifier = nn.Linear(lstm_hidden_size, num_labels)\n",
    "        \n",
    "        # CRF layer\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get token-level representations\n",
    "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Apply BiLSTM\n",
    "        lstm_output, _ = self.lstm(sequence_output)  # [batch_size, seq_len, 2*hidden_size]\n",
    "        \n",
    "        # Apply classification layers\n",
    "        x = self.dropout(lstm_output)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation(x)\n",
    "        if self.use_layer_norm:\n",
    "            x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        emissions = self.classifier(x)  # [batch_size, seq_len, num_labels]\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Create mask for CRF\n",
    "            crf_mask = attention_mask.bool()\n",
    "            \n",
    "            # CRF loss (negative log-likelihood)\n",
    "            loss = -self.crf(emissions, labels, mask=crf_mask, reduction='mean')\n",
    "        \n",
    "        # CRF decoding for predictions\n",
    "        predictions = self.crf.decode(emissions, mask=attention_mask.bool())\n",
    "        # Convert list of lists to tensor with padding\n",
    "        max_len = emissions.size(1)\n",
    "        pred_tensor = torch.zeros_like(input_ids)\n",
    "        for i, pred_seq in enumerate(predictions):\n",
    "            pred_tensor[i, :len(pred_seq)] = torch.tensor(pred_seq, device=pred_tensor.device)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': emissions,\n",
    "            'predictions': pred_tensor\n",
    "        }\n",
    "\n",
    "# Post-processing function\n",
    "def post_process_bio_tags(tokens, tags, token_is_first_subword):\n",
    "    \"\"\"\n",
    "    Apply linguistic rules to fix common errors in BIO tag sequences\n",
    "    \n",
    "    Parameters:\n",
    "    - tokens: List of tokens (including subtokens)\n",
    "    - tags: Predicted BIO tags (0=O, 1=B-IDIOM, 2=I-IDIOM)\n",
    "    - token_is_first_subword: Boolean list indicating if a token is the first subword of a word\n",
    "    \n",
    "    Returns:\n",
    "    - Corrected BIO tags\n",
    "    \"\"\"\n",
    "    corrected_tags = tags.copy()\n",
    "    \n",
    "    # Rule 1: Fix I-IDIOM without preceding B-IDIOM\n",
    "    for i in range(len(tags)):\n",
    "        if i > 0 and token_is_first_subword[i] and tags[i] == 2 and tags[i-1] == 0:  # I-IDIOM after O\n",
    "            # Either correct to B-IDIOM or O\n",
    "            if i < len(tags)-1 and tags[i+1] == 2:  # If followed by I-IDIOM\n",
    "                corrected_tags[i] = 1  # Convert to B-IDIOM\n",
    "            else:\n",
    "                corrected_tags[i] = 0  # Convert to O if isolated\n",
    "    \n",
    "    # Rule 2: Fix consecutive B-IDIOM tags (should usually be B-IDIOM followed by I-IDIOM)\n",
    "    for i in range(len(tags)-1):\n",
    "        if token_is_first_subword[i] and token_is_first_subword[i+1] and tags[i] == 1 and tags[i+1] == 1:\n",
    "            corrected_tags[i+1] = 2  # Convert second to I-IDIOM\n",
    "    \n",
    "    # Rule 3: Fix B-IDIOM without any following I-IDIOM and not followed by a special token\n",
    "    # This might be a model error for very short expressions\n",
    "    for i in range(len(tags)-1):\n",
    "        if token_is_first_subword[i] and token_is_first_subword[i+1] and tags[i] == 1 and tags[i+1] == 0:\n",
    "            # Check context to determine if it's likely part of an idiom\n",
    "            if i+2 < len(tags) and tags[i+2] == 2:  # Pattern O, B, O, I\n",
    "                corrected_tags[i+1] = 2  # Make the O between B and I also I\n",
    "    \n",
    "    return corrected_tags\n",
    "\n",
    "def apply_post_processing(model, tokenizer, input_ids, attention_mask, device):\n",
    "    \"\"\"Apply model prediction and post-processing to input sequence\"\"\"\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    preds = outputs['predictions'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    masks = attention_mask[0]\n",
    "    \n",
    "    # Identify which tokens are first subwords\n",
    "    token_is_first_subword = [True]  # For CLS token\n",
    "    for token in tokens[1:]:  # Skip CLS token\n",
    "        if token in ['[SEP]', '[PAD]']:\n",
    "            token_is_first_subword.append(True)  # Special tokens\n",
    "        else:\n",
    "            token_is_first_subword.append(not token.startswith('##'))\n",
    "    \n",
    "    # Apply post-processing\n",
    "    corrected_preds = post_process_bio_tags(tokens, preds.tolist(), token_is_first_subword)\n",
    "    \n",
    "    # Map back to tensor\n",
    "    corrected_tensor = torch.tensor(corrected_preds, device=preds.device)\n",
    "    \n",
    "    return corrected_tensor, tokens, masks\n",
    "\n",
    "def predict_idioms_with_postprocessing(model, tokenizer, sentence, device, language=None):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Apply model and post-processing\n",
    "    corrected_preds, tokens, masks = apply_post_processing(\n",
    "        model, tokenizer, input_ids, attention_mask, device\n",
    "    )\n",
    "    \n",
    "    # Convert to word-level predictions\n",
    "    words = []\n",
    "    bio_tags = []\n",
    "    word_idx = -1\n",
    "    idiom_indices = []\n",
    "    current_idiom = []\n",
    "    current_word = \"\"\n",
    "    previous_tag = 0  # O tag\n",
    "    \n",
    "    for token, mask, pred in zip(tokens, masks, corrected_preds):\n",
    "        if mask == 0 or token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "            \n",
    "        if not token.startswith('##'):  # New word\n",
    "            # Save previous word\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "                bio_tags.append(previous_tag)\n",
    "                word_idx += 1\n",
    "                \n",
    "                # Handle idiom tracking\n",
    "                if previous_tag in [1, 2] and pred.item() not in [1, 2]:  # End of idiom\n",
    "                    if current_idiom:\n",
    "                        idiom_indices.extend(current_idiom)\n",
    "                        current_idiom = []\n",
    "            \n",
    "            # Start new word\n",
    "            current_word = token\n",
    "            previous_tag = pred.item()\n",
    "            \n",
    "            # Track idioms\n",
    "            if pred.item() == 1:  # B-IDIOM\n",
    "                current_idiom = [word_idx + 1]  # +1 because we haven't incremented yet\n",
    "            elif pred.item() == 2:  # I-IDIOM\n",
    "                if previous_tag in [1, 2]:  # Continue idiom\n",
    "                    current_idiom.append(word_idx + 1)\n",
    "        else:\n",
    "            # Continue current word\n",
    "            current_word += token[2:]  # Remove ## prefix\n",
    "    \n",
    "    # Don't forget last word\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "        bio_tags.append(previous_tag)\n",
    "        \n",
    "        # Handle last idiom\n",
    "        if previous_tag in [1, 2] and current_idiom:\n",
    "            idiom_indices.extend(current_idiom)\n",
    "    \n",
    "    # Apply language-specific post-processing if language is provided\n",
    "    if language == 'italian':\n",
    "        # Add Italian-specific rules here if needed\n",
    "        pass\n",
    "    elif language == 'turkish':\n",
    "        # Add Turkish-specific rules here if needed\n",
    "        pass\n",
    "    \n",
    "    return idiom_indices\n",
    "\n",
    "def evaluate(model, val_loader, tokenizer, device, apply_postprocessing=True):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    total_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            if batch['input_ids'].size(0) == 0:\n",
    "                continue\n",
    "                \n",
    "            total_batches += 1\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass with loss calculation\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            if outputs['loss'] is not None:\n",
    "                val_loss += outputs['loss'].item()\n",
    "            \n",
    "            preds = outputs['predictions']\n",
    "            \n",
    "            if apply_postprocessing:\n",
    "                # Apply post-processing to each sequence\n",
    "                processed_preds = []\n",
    "                for i in range(preds.size(0)):\n",
    "                    # Identify which tokens are first subwords for this sequence\n",
    "                    tokens = tokenizer.convert_ids_to_tokens(input_ids[i])\n",
    "                    token_is_first_subword = [True]  # For CLS token\n",
    "                    for token in tokens[1:]:  # Skip CLS token\n",
    "                        if token in ['[SEP]', '[PAD]']:\n",
    "                            token_is_first_subword.append(True)  # Special tokens\n",
    "                        else:\n",
    "                            token_is_first_subword.append(not token.startswith('##'))\n",
    "                    \n",
    "                    # Apply post-processing\n",
    "                    corrected_seq = post_process_bio_tags(tokens, preds[i].tolist(), token_is_first_subword)\n",
    "                    processed_preds.append(torch.tensor(corrected_seq, device=preds.device))\n",
    "                \n",
    "                # Replace original predictions with processed ones\n",
    "                preds = torch.stack(processed_preds)\n",
    "            \n",
    "            # Process each sequence in batch (same as original evaluate function)\n",
    "            for seq_preds, seq_mask, seq_labels, seq_ids in zip(preds, attention_mask, labels, input_ids):\n",
    "                tokens = tokenizer.convert_ids_to_tokens(seq_ids)\n",
    "                \n",
    "                # Extract idiom indices based on BIO tags\n",
    "                # For ground truth\n",
    "                word_idx = -1\n",
    "                true_idiom_indices = []\n",
    "                current_idiom_indices = []\n",
    "                previous_tag = 0  # O tag\n",
    "                \n",
    "                for i, (token, mask, label) in enumerate(zip(tokens, seq_mask, seq_labels)):\n",
    "                    if mask == 0 or token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                        continue\n",
    "                        \n",
    "                    if not token.startswith('##'):  # New word\n",
    "                        word_idx += 1\n",
    "                        \n",
    "                        # Handle end of previous idiom\n",
    "                        if previous_tag in [1, 2] and label.item() not in [1, 2]:\n",
    "                            # End of idiom\n",
    "                            if current_idiom_indices:\n",
    "                                true_idiom_indices.extend(current_idiom_indices)\n",
    "                                current_idiom_indices = []\n",
    "                        \n",
    "                        # Handle new idiom\n",
    "                        if label.item() == 1:  # B-IDIOM\n",
    "                            current_idiom_indices = [word_idx]\n",
    "                        elif label.item() == 2:  # I-IDIOM\n",
    "                            if previous_tag in [1, 2]:  # Continue idiom\n",
    "                                current_idiom_indices.append(word_idx)\n",
    "                        \n",
    "                        previous_tag = label.item()\n",
    "                \n",
    "                # Don't forget last idiom\n",
    "                if current_idiom_indices:\n",
    "                    true_idiom_indices.extend(current_idiom_indices)\n",
    "                \n",
    "                # For predictions\n",
    "                word_idx = -1\n",
    "                pred_idiom_indices = []\n",
    "                current_idiom_indices = []\n",
    "                previous_tag = 0  # O tag\n",
    "                \n",
    "                for i, (token, mask, pred) in enumerate(zip(tokens, seq_mask, seq_preds)):\n",
    "                    if mask == 0 or token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                        continue\n",
    "                        \n",
    "                    if not token.startswith('##'):  # New word\n",
    "                        word_idx += 1\n",
    "                        \n",
    "                        # Handle end of previous idiom\n",
    "                        if previous_tag in [1, 2] and pred.item() not in [1, 2]:\n",
    "                            # End of idiom\n",
    "                            if current_idiom_indices:\n",
    "                                pred_idiom_indices.extend(current_idiom_indices)\n",
    "                                current_idiom_indices = []\n",
    "                        \n",
    "                        # Handle new idiom\n",
    "                        if pred.item() == 1:  # B-IDIOM\n",
    "                            current_idiom_indices = [word_idx]\n",
    "                        elif pred.item() == 2:  # I-IDIOM\n",
    "                            if previous_tag in [1, 2]:  # Continue idiom\n",
    "                                current_idiom_indices.append(word_idx)\n",
    "                        \n",
    "                        previous_tag = pred.item()\n",
    "                \n",
    "                # Don't forget last idiom\n",
    "                if current_idiom_indices:\n",
    "                    pred_idiom_indices.extend(current_idiom_indices)\n",
    "                \n",
    "                # Store results\n",
    "                predictions.append(pred_idiom_indices)\n",
    "                ground_truth.append(true_idiom_indices)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_val_loss = val_loss / max(1, total_batches)\n",
    "    \n",
    "    # Calculate F1 scores using competition method\n",
    "    f1_scores = []\n",
    "    for pred, gold in zip(predictions, ground_truth):\n",
    "        # Handle special case for no idiom\n",
    "        if not gold:  # Empty gold = no idiom\n",
    "            if not pred:  # Empty pred = correctly predicted no idiom\n",
    "                f1_scores.append(1.0)\n",
    "            else:\n",
    "                f1_scores.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        # Normal case - set comparison\n",
    "        pred_set = set(pred)\n",
    "        gold_set = set(gold)\n",
    "        \n",
    "        intersection = len(pred_set & gold_set)\n",
    "        precision = intersection / len(pred_set) if len(pred_set) > 0 else 0\n",
    "        recall = intersection / len(gold_set) if len(gold_set) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    mean_f1 = sum(f1_scores) / max(1, len(f1_scores))\n",
    "    \n",
    "    print(f\"\\nValidation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_val_loss,\n",
    "        'f1': mean_f1,\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth\n",
    "    }\n",
    "\n",
    "def train_model(train_loader, val_loader, tokenizer, model=None, epochs=10, lr=2e-5, \n",
    "                weight_decay=0.01, lr_multiplier=10, patience=3):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create model if not provided\n",
    "    if model is None:\n",
    "        model = EnhancedBertForIdiomDetection().to(device)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Differential learning rates\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() \n",
    "                       if not any(nd in n for nd in no_decay) and 'bert' in n and p.requires_grad],\n",
    "            'weight_decay': weight_decay,\n",
    "            'lr': lr\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() \n",
    "                       if any(nd in n for nd in no_decay) and 'bert' in n and p.requires_grad],\n",
    "            'weight_decay': 0.0,\n",
    "            'lr': lr\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() \n",
    "                       if not any(nd in n for nd in no_decay) and 'bert' not in n],\n",
    "            'weight_decay': weight_decay,\n",
    "            'lr': lr * lr_multiplier\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() \n",
    "                       if any(nd in n for nd in no_decay) and 'bert' not in n],\n",
    "            'weight_decay': 0.0,\n",
    "            'lr': lr * lr_multiplier\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
    "    \n",
    "        # Learning rate scheduler with warmup\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=[lr, lr, lr*lr_multiplier, lr*lr_multiplier],\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.1  # 10% warmup\n",
    "    )\n",
    "    \n",
    "    best_f1 = 0\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs['loss']\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluation\n",
    "        metrics = evaluate(model, val_loader, tokenizer, device, apply_postprocessing=True)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {metrics['loss']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        \n",
    "        # Save best model and check for early stopping\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            torch.save(model.state_dict(), \"best_idiom_model.pt\")\n",
    "            print(\"New best model saved!\")\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            print(f\"No improvement for {no_improve_epochs} epochs\")\n",
    "            \n",
    "            if no_improve_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(torch.load(\"best_idiom_model.pt\"))\n",
    "    return model\n",
    "\n",
    "# Now update the main functions for training, evaluation, and prediction\n",
    "\n",
    "def run_train(epochs=10, lr=2e-5, batch_size=8, max_length=128, \n",
    "              lstm_hidden_size=384, lstm_layers=2, lstm_dropout=0.3,\n",
    "              hidden_dropout=0.3, use_layer_norm=True, freeze_bert_layers=0,\n",
    "              weight_decay=0.01, lr_multiplier=10, patience=3):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Get data loaders\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        batch_size=batch_size, \n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    \n",
    "    # Initialize model with optimal parameters\n",
    "    model = EnhancedBertForIdiomDetection(\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_layers=lstm_layers,\n",
    "        lstm_dropout=lstm_dropout,\n",
    "        hidden_dropout=hidden_dropout,\n",
    "        use_layer_norm=use_layer_norm,\n",
    "        freeze_bert_layers=freeze_bert_layers\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model = train_model(\n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        lr_multiplier=lr_multiplier,\n",
    "        patience=patience\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'final_idiom_model.pt')\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_eval(batch_size=8, max_length=128, \n",
    "             lstm_hidden_size=384, lstm_layers=2, lstm_dropout=0.3,\n",
    "             hidden_dropout=0.3, use_layer_norm=True, freeze_bert_layers=0,\n",
    "             model_path='best_idiom_model.pt'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Get data loaders\n",
    "    _, val_loader, _ = get_dataloaders(\n",
    "        batch_size=batch_size, \n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    \n",
    "    # Initialize model with the same parameters\n",
    "    model = EnhancedBertForIdiomDetection(\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_layers=lstm_layers,\n",
    "        lstm_dropout=lstm_dropout,\n",
    "        hidden_dropout=hidden_dropout,\n",
    "        use_layer_norm=use_layer_norm,\n",
    "        freeze_bert_layers=freeze_bert_layers\n",
    "    )\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    \n",
    "    # Evaluate the model with post-processing\n",
    "    metrics = evaluate(model, val_loader, tokenizer, device, apply_postprocessing=True)\n",
    "    \n",
    "    print(f\"Evaluation complete!\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def run_predict(output='predictions.csv', \n",
    "                lstm_hidden_size=384, lstm_layers=2, lstm_dropout=0.3,\n",
    "                hidden_dropout=0.3, use_layer_norm=True, freeze_bert_layers=0,\n",
    "                model_path='best_idiom_model.pt'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model with the same parameters\n",
    "    model = EnhancedBertForIdiomDetection(\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_layers=lstm_layers,\n",
    "        lstm_dropout=lstm_dropout,\n",
    "        hidden_dropout=hidden_dropout,\n",
    "        use_layer_norm=use_layer_norm,\n",
    "        freeze_bert_layers=freeze_bert_layers\n",
    "    )\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    # Read test data\n",
    "    test_df = pd.read_csv('public_data/eval_w_o_labels.csv')\n",
    "    ids = test_df['id'].tolist()\n",
    "    sentences = test_df['sentence'].tolist()\n",
    "    languages = test_df['language'].tolist()\n",
    "\n",
    "    results = []\n",
    "    for idx, sentence, lang in zip(ids, sentences, languages):\n",
    "        # Use the new prediction function with post-processing\n",
    "        idiom_indices = predict_idioms_with_postprocessing(\n",
    "            model, tokenizer, sentence, device, language=lang\n",
    "        )\n",
    "        \n",
    "        # If no idiom is found, use [-1] as per the competition format\n",
    "        if not idiom_indices:\n",
    "            idiom_indices = [-1]\n",
    "            \n",
    "        # Format the indices as a string for submission\n",
    "        indices_str = str(idiom_indices).replace(' ', '')\n",
    "        \n",
    "        results.append({\n",
    "            'id': idx,\n",
    "            'indices': indices_str,\n",
    "            'language': lang\n",
    "        })\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    out_df = pd.DataFrame(results)\n",
    "    out_df.to_csv(output, index=False)\n",
    "    print(f'Predictions saved to {output}')\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "def run_hyperparameter_search(n_trials=5, batch_size=8, max_length=128):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Get data loaders\n",
    "    train_loader, val_loader, _ = get_dataloaders(\n",
    "        batch_size=batch_size, \n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    \n",
    "    # Define the hyperparameter search space\n",
    "    param_grid = {\n",
    "        'learning_rate': [1e-5, 2e-5, 3e-5, 5e-5],\n",
    "        'lstm_hidden_size': [256, 384, 512],\n",
    "        'lstm_layers': [1, 2, 3],\n",
    "        'lstm_dropout': [0.2, 0.3, 0.4],\n",
    "        'hidden_dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "        'use_layer_norm': [True, False],\n",
    "        'freeze_bert_layers': [0, 3, 6],\n",
    "        'weight_decay': [0.0, 0.01, 0.05],\n",
    "        'lr_multiplier': [5, 10, 15]\n",
    "    }\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_config = None\n",
    "    results = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        print(f\"\\n===== Trial {trial+1}/{n_trials} =====\")\n",
    "        \n",
    "        # Sample random hyperparameters and convert to Python native types\n",
    "        config = {\n",
    "            'learning_rate': float(np.random.choice(param_grid['learning_rate'])),\n",
    "            'lstm_hidden_size': int(np.random.choice(param_grid['lstm_hidden_size'])),\n",
    "            'lstm_layers': int(np.random.choice(param_grid['lstm_layers'])),\n",
    "            'lstm_dropout': float(np.random.choice(param_grid['lstm_dropout'])),\n",
    "            'hidden_dropout': float(np.random.choice(param_grid['hidden_dropout'])),\n",
    "            'use_layer_norm': bool(np.random.choice(param_grid['use_layer_norm'])),\n",
    "            'freeze_bert_layers': int(np.random.choice(param_grid['freeze_bert_layers'])),\n",
    "            'weight_decay': float(np.random.choice(param_grid['weight_decay'])),\n",
    "            'lr_multiplier': int(np.random.choice(param_grid['lr_multiplier']))\n",
    "        }\n",
    "        \n",
    "        print(\"Configuration:\")\n",
    "        for k, v in config.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        \n",
    "        # Initialize model with sampled hyperparameters\n",
    "        model = EnhancedBertForIdiomDetection(\n",
    "            lstm_hidden_size=config['lstm_hidden_size'],\n",
    "            lstm_layers=config['lstm_layers'],\n",
    "            lstm_dropout=config['lstm_dropout'],\n",
    "            hidden_dropout=config['hidden_dropout'],\n",
    "            use_layer_norm=config['use_layer_norm'],\n",
    "            freeze_bert_layers=config['freeze_bert_layers']\n",
    "        )\n",
    "        \n",
    "        # Train for a few epochs to evaluate the configuration\n",
    "        trial_epochs = 3  # Use fewer epochs for quick evaluation\n",
    "        model = train_model(\n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            tokenizer,\n",
    "            model=model,\n",
    "            epochs=trial_epochs,\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            lr_multiplier=config['lr_multiplier'],\n",
    "            patience=2  # Use shorter patience for hyperparameter search\n",
    "        )\n",
    "        \n",
    "        # Evaluate with post-processing\n",
    "        metrics = evaluate(model, val_loader, tokenizer, device, apply_postprocessing=True)\n",
    "        f1_score = metrics['f1']\n",
    "        \n",
    "        print(f\"Trial {trial+1} F1 Score: {f1_score:.4f}\")\n",
    "        \n",
    "        # Save results\n",
    "        config['f1_score'] = f1_score\n",
    "        results.append(config)\n",
    "        \n",
    "        # Update best configuration\n",
    "        if f1_score > best_f1:\n",
    "            best_f1 = f1_score\n",
    "            best_config = config\n",
    "            print(f\"New best configuration found! F1: {best_f1:.4f}\")\n",
    "    \n",
    "    # Print results summary\n",
    "    print(\"\\n===== Hyperparameter Search Results =====\")\n",
    "    print(f\"Best F1 Score: {best_f1:.4f}\")\n",
    "    print(\"Best Configuration:\")\n",
    "    for k, v in best_config.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Sort all results by F1 score\n",
    "    results.sort(key=lambda x: x['f1_score'], reverse=True)\n",
    "    print(\"\\nTop 3 configurations:\")\n",
    "    for i, config in enumerate(results[:3]):\n",
    "        print(f\"Rank {i+1}: F1={config['f1_score']:.4f}\")\n",
    "        for k, v in config.items():\n",
    "            if k != 'f1_score':\n",
    "                print(f\"  {k}: {v}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "# Function to run full pipeline with best hyperparameters\n",
    "def run_full_pipeline(n_trials=5, final_epochs=10, output='predictions.csv'):\n",
    "    # Step 1: Find optimal hyperparameters\n",
    "    print(\"Starting hyperparameter search...\")\n",
    "    best_config = run_hyperparameter_search(n_trials=n_trials)\n",
    "    \n",
    "    # Step 2: Train the final model with the best configuration\n",
    "    # Ensure all numeric values are Python native types\n",
    "    print(\"\\nTraining final model with the best configuration...\")\n",
    "    final_model = run_train(\n",
    "        epochs=final_epochs,\n",
    "        lr=float(best_config['learning_rate']),\n",
    "        batch_size=8,\n",
    "        max_length=128,\n",
    "        lstm_hidden_size=int(best_config['lstm_hidden_size']),\n",
    "        lstm_layers=int(best_config['lstm_layers']),\n",
    "        lstm_dropout=float(best_config['lstm_dropout']),\n",
    "        hidden_dropout=float(best_config['hidden_dropout']),\n",
    "        use_layer_norm=bool(best_config['use_layer_norm']),\n",
    "        freeze_bert_layers=int(best_config['freeze_bert_layers']),\n",
    "        weight_decay=float(best_config['weight_decay']),\n",
    "        lr_multiplier=int(best_config['lr_multiplier'])\n",
    "    )\n",
    "    \n",
    "    # Step 3: Evaluate the final model\n",
    "    print(\"\\nEvaluating the final model...\")\n",
    "    metrics = run_eval(\n",
    "        batch_size=8,\n",
    "        max_length=128,\n",
    "        lstm_hidden_size=best_config['lstm_hidden_size'],\n",
    "        lstm_layers=best_config['lstm_layers'],\n",
    "        lstm_dropout=best_config['lstm_dropout'],\n",
    "        hidden_dropout=best_config['hidden_dropout'],\n",
    "        use_layer_norm=best_config['use_layer_norm'],\n",
    "        freeze_bert_layers=best_config['freeze_bert_layers']\n",
    "    )\n",
    "    \n",
    "    # Step 4: Generate predictions\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    predictions = run_predict(\n",
    "        output=output,\n",
    "        lstm_hidden_size=best_config['lstm_hidden_size'],\n",
    "        lstm_layers=best_config['lstm_layers'],\n",
    "        lstm_dropout=best_config['lstm_dropout'],\n",
    "        hidden_dropout=best_config['hidden_dropout'],\n",
    "        use_layer_norm=best_config['use_layer_norm'],\n",
    "        freeze_bert_layers=best_config['freeze_bert_layers']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFull pipeline completed successfully!\")\n",
    "    print(f\"Final F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Predictions saved to: {output}\")\n",
    "    \n",
    "    return {\n",
    "        'best_config': best_config,\n",
    "        'final_f1': metrics['f1'],\n",
    "        'predictions': predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "run_eval()\n",
    "\n",
    "# Generate predictions\n",
    "run_predict(output='predictions.csv')\n",
    "\n",
    "# Or run the full pipeline with hyperparameter search\n",
    "run_full_pipeline(n_trials=5, final_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
